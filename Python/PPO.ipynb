{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import MultivariateNormal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize weights of NN from normal distribution\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Neural Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, inputs, outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(inputs, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(inputs, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, outputs),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(outputs) * std)\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = None\n",
    "        self.group_name = None\n",
    "        self.group_spec = None\n",
    "        self.agents = 0\n",
    "        self.observations = 0\n",
    "        self.actions = 0\n",
    "    \n",
    "    # Connect to Unity VE\n",
    "    def connect_env(self):\n",
    "        for i in range(11):\n",
    "            try:\n",
    "                self.env = UnityEnvironment(file_name=None, base_port=5004)\n",
    "                self.env.reset()\n",
    "                self.group_name = self.env.get_agent_groups()[0] \n",
    "                self.group_spec = self.env.get_agent_group_spec(self.group_name)\n",
    "                step_result = self.env.get_step_result(self.group_name)\n",
    "                self.agents = step_result.n_agents()\n",
    "                print('Connected...')\n",
    "                return self.group_name, self.group_spec\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "                self.env = None\n",
    "                self.group_name = None\n",
    "                self.group_spec = None\n",
    "        return self.group_name, self.group_spec\n",
    "    \n",
    "    # Close environment\n",
    "    def close_env(self):\n",
    "        try:\n",
    "            self.env.close()\n",
    "        except:\n",
    "            pass\n",
    "        return\n",
    "    \n",
    "    # Get space dimensions\n",
    "    def get_shapes(self):\n",
    "        try:\n",
    "            self.observations = self.group_spec.observation_shapes\n",
    "            self.actions = self.group_spec.action_shape\n",
    "        except:\n",
    "            self.observations = None\n",
    "            self.actions = None\n",
    "        return self.observations, self.actions\n",
    "    \n",
    "    # Get observations\n",
    "    def __result(self):\n",
    "        step_result = self.env.get_step_result(self.group_name)\n",
    "        sAgents = step_result.n_agents()\n",
    "        if (sAgents == self.agents):\n",
    "            obs = step_result.obs[0]\n",
    "        else:\n",
    "            obs = step_results.obs[0][0:self.agents]\n",
    "        return obs\n",
    "    \n",
    "    # Get reward\n",
    "    def __reward(self):\n",
    "        step_result = self.env.get_step_result(self.group_name)\n",
    "        sAgents = step_result.n_agents()\n",
    "        if (sAgents == self.agents):\n",
    "            reward = step_result.reward\n",
    "        else:\n",
    "            reward = step_result.reward[0:self.agents]\n",
    "        return reward\n",
    "    \n",
    "    # Get done\n",
    "    def __done(self):\n",
    "        step_result = self.env.get_step_result(self.group_name)\n",
    "        sAgents = step_result.n_agents()\n",
    "        if (sAgents == self.agents):\n",
    "            done = step_result.done\n",
    "        else:\n",
    "            done = step_result.done[0:self.agents] \n",
    "        return done\n",
    "    \n",
    "    # Reset\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        result, reward, done = self.__result(), self.__reward(), self.__done()\n",
    "        return result, reward, done        \n",
    "    \n",
    "    # Step\n",
    "    def step(self, action):\n",
    "        step_result = self.env.get_step_result(self.group_name)\n",
    "        sAgents = step_result.n_agents()\n",
    "        action = np.array(action.cpu())\n",
    "        print(action)\n",
    "        if (sAgents == self.agents):\n",
    "            self.env.set_actions(self.group_name, action)\n",
    "        else:\n",
    "            x = np.full((sAgents, action.shape[0]), action)\n",
    "            self.env.set_actions(self.group_name, x)\n",
    "        self.env.step()\n",
    "        result, reward, done = self.__result(), self.__reward(), self.__done()\n",
    "        return result, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, inputs, outputs, neurons, lr_):\n",
    "        super(PPO, self).__init__()\n",
    "        self.model = ActorCritic(inputs, outputs, neurons)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr_)\n",
    "    \n",
    "    # Generalized Advantage Estimator\n",
    "    def gae(self, next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "        values = values + [next_value]\n",
    "        g = 0\n",
    "        returns = []\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "            g = delta + gamma * tau * g * masks[step]\n",
    "            returns.insert(0, g + values[step])\n",
    "        return returns\n",
    "    \n",
    "    # Mini-batch iteration PPO\n",
    "    def batch(self, mini_batch, states, actions, \n",
    "                       log_probs, returns, advantage):\n",
    "        batch_size = states.size(0)\n",
    "        print(batch_size//mini_batch)\n",
    "        for _ in range(batch_size // mini_batch):\n",
    "            ids = np.random.randint(0, batch_size, mini_batch)\n",
    "            yield states[ids, :], actions[ids, :], log_probs[ids, :], returns[ids, :], advantage[ids, :]\n",
    "            \n",
    "    # Update PPO\n",
    "    def update(epochs, mini_batch, states, actions, log_probs,\n",
    "              returns, advantages, epsilon=0.2):\n",
    "        for _ in range(epochs):\n",
    "            for state, action, old_probs, ret, adv in self.batch(mini_batch, states, actions, log_probs, returns, advantages):\n",
    "                dist, value = self.model(state)\n",
    "                entropy = dist.entropy().mean()\n",
    "                new_probs = dist.log_prob(action)\n",
    "                \n",
    "                ratio = (new_probs - old_probs).exp()\n",
    "                surr1 = ratio * adv\n",
    "                surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * adv\n",
    "                \n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = (ret - value).pow(2).mean()\n",
    "                \n",
    "                loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PPO()\n",
    "x = [1 for i in range(10)]\n",
    "r = p.gae(10.0, x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
